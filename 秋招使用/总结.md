# 项目
# 实习
## KVM侧的halt pollig性能测试
kvm侧的halt polling是指在虚拟机空闲时，vm exit后，kvm侧会polling一段时间，为了让虚拟机再次被唤醒的时候能立刻运行，但是这个还是会存在vm exit的开销，这个halt polling system节省的是执行调度程序的时间。   
guest侧的halt polling driver相当于在vm exit之前进行polling，这样就不会因虚拟机空闲产生vm exit。   
kvm侧的halt polling：节省了调度的时间。如果host侧有任务的话会立即调度   
guest侧的halt polling：相对于kvm侧的halt polling来说可能更高效，因为不会产生vm exit，但是缺点是，如果host侧有需要运行的任务，guest侧依然会polling
## ARM架构支持capabilities显示vendor等
在src/conf/capabilities.c文件中是输出这个的   
ARM CPU的CPU型号、Vendor、版本等信息存于MIDR_EL1寄存器中，所以直接读取这个寄存器并解析即可
``` C
asm("mrs %0, MIDR_EL1" : "=r" (your_para));
```
mrs和msr指令，读写寄存器，只有MRS和MSR指令可以对状态寄存器CPSR（**程序状态寄存器**）和SPSR（**序状态保存寄存器。SPSR用于保存CPSR的状态**）进行读写操作
## guest侧halt polling
cpuidle core 是 cpuidle framework 的核心模块，负责抽象出 cpuidle device、cpuidle driver 和 cpuidle governor 三个实体，其中cpuidle device是虚拟设备，对应一个CPU，针对每个CPU核都对应一个struct cpuidle_device结构；cpuidle driver是用于驱动一个或多个CPU核；cpu governor用于选择不同的策略   
![图片](./ima/cpu%20idle.PNG "cpuidle")
ARM下的WFI WFE指令执行idle，x86下的halt指令   
governor的策略有ladder和menu，ladder是梯度的慢慢进入更省电状态，menu可以选择   
我们的实现是：注册一个cpuidle_driver，然后这个的cpuidle_state添加一个polling再执行default_idle函数（默认的第0个是直接default idle），然后注册一个governor，由它进行选择   
关于怎么执行到idle，那就是0号进程，当当前CPU队列没有任务执行时，就执行idle
## 其他
### KVM和qemu
kvm是内核模块，可以将物理cpu虚拟化为vcpu，但是它并不能虚拟化I/O等。单靠内核中的kvm模块并不能启动一台虚拟机，其只能模拟vcpu,vmemory, 像io设备的模拟还需要借助用户空间程序qemu    
### Linux transparent huge pages (THP)透明大页
在 Linux 操作系统上运行内存需求量较大的应用程序时，由于其采用的默认页面大小为 4KB，因而将会产生较多 TLB Miss 和缺页中断，从而大大影响应用程序的性能。当操作系统以 2MB 甚至更大作为分页的单位时，将会大大减少 TLB Miss 和缺页中断的数量，显著提高应用程序的性能。这也正是 Linux 内核引入大页面支持的直接原因。好处是很明显的，假设应用程序需要 2MB 的内存，如果操作系统以 4KB 作为分页的单位，则需要 512 个页面，进而在 TLB 中需要 512 个表项，同时也需要 512 个页表项，操作系统需要经历至少 512 次 TLB Miss 和 512 次缺页中断才能将 2MB 应用程序空间全部映射到物理内存。然而，当操作系统采用 2MB 作为分页的基本单位时，只需要一次 TLB Miss 和一次缺页中断，就可以为 2MB 的应用程序空间建立虚实映射，并在运行过程中无需再经历 TLB Miss 和缺页中断（假设未发生 TLB 项替换和 Swap）。   
为了能以最小的代价实现大页面支持，Linux 操作系统采用了基于 hugetlbfs 特殊文件系统 2M 字节大页面支持。这种采用特殊文件系统形式支持大页面的方式，使得应用程序可以根据需要灵活地选择虚存页面大小，而不会被强制使用 2MB 大页面。
### DMA和IOMMU
出现DMA的原因：设备向内存复制数据都需要经过CPU的话，太浪费CPU的性能了。   
同步DMA：DMA操作由软件发起，一般流程是设备驱动设置好需要被DMA访问的内存地址后，通过写寄存器通知设备发起DMA操作。比如，播放音频时，驱动将该音频存的地址告知声卡，设备从内存读取数据并播放，播放完成后中断通知驱动   
异步DMA：DMA操作由设备发起。比如网卡，网卡收到的数据直接复制到驱动设置的内存地址去，中断形式通知网络数据包的到来。   
DMA操作的是物理地址，所以，虚拟化情况下可能会受到攻击，所以出现了IOMMU。
### MMIO和port I/O
就是设备的独立编址和统一编址   
Port I/O：使用IO端口访问设备，IN OUT指令等   
MMIO：通过内存访问形式访问设备寄存器或设备RAM
### 使用free命令或者cat /proc/meminfo显示的内存比实际分配的小
ree或者是cat /proc/meminfo看到的总内存是伙伴系统可以管理的所有内存的总和，也就是操作系统起来后可供内核或者内核模块以及应用程序分配的内存总和，它不包括BIOS预留的作为特殊用途的内存，也不包括内核代码、为kdump预留， 管理内存页的元数据（struct page）等内存，虽然这些内存不可见，但却是实实在在被虚拟机使用了

### 一些命令
```Shell
lscpu  ##cpu情况
lsmod ##查看mod安装情况
free ##查看磁盘
top ## cpu利用率，，，，按1可以看各CPU单独利用率
df -hl  ##文件系统占用空间， -hl使用
du -hl --max-depth=1 ##查看占用磁盘空间
ifconfig 
cat /proc/cmdline
cat /proc/version ## 查看linux版本
/proc/cpuinfo ##cpu的信息
/sys/modules ##模块
perf stat -e 'kvm:*' -a sleep 10 ##kvm 10秒钟状态
gdb
systemtap ##后面需要看看

###########一些virsh命令
visrsh define wfs.xml #定义
virsh start wfs  #启动
virsh list   #查看列表           --all 查看所有的
virsh dominfo wfs  #显示虚拟机信息
virsh console wfs    #通过控制窗口登录虚拟机   exit退出之后   “ctrl+]”  退出
```
# Linux
## CPU缓存
出现目的：解决CPU和内存之间速度不匹配   
一般一个缓存行大小是64字节   
缓存的意义满足以下原理：**时间局部性原理**和**空间局部性原理**   
三级缓存：L1、L2、L3   L1分为l1i和l1d，指令和数据分开，l3缓存是所有CPU共享的   
**MESI协议** 修改 独占 共享 无效      
MESI保证了一致性，但是需要对状态进行更改，消息传递需要时间。比如修改了本地缓存，那么必须把I状态传递给其他拥有该缓存数据的CPU并等待确认，这个过程会阻塞CPU，影响性能。所以引入了**存储缓存**和**无效队列**   
**存储缓存**：异步处理，用一个buffer，先把值写道buffer中，再发送信号，等确认后再应用到cache中   
**无效队列**：接受端 CPU 接受到 Invalidate 信号后如果立即采取相应行动(去其它 CPU 同步值)，再返回响应信号，则时钟周期也太长了，此处也可优化。接受端 CPU 接受到信号后不是立即采取行动，而是将 Invalidate 信号插入到一个队列 Queue 中，立即作出响应。等到合适的时机，再去处理这个 Queue 中的 Invalidate 信号，并作相应处理

乱序执行
## IO与网络模型
![图片](./ima/IO.png)   
**原子变量** 多核竞争数据总线时    
**自旋锁** 循环，适用于很快能获得处理器资源的任务，不能用在中断上下文中    
**互斥锁** 同一时刻只有一个线程进入临界区，其他的休眠    
**信号量** 保护有限数量的临界资源，使用自旋锁保护   
**读写锁** 减小加锁粒度，读写分别加锁，优化了读大于写的场景   
**preempt抢占** 时间片用完后调用schedule、IO等原因自己调用schedule、当前进程被其他进程替换   
**per-cpu变量** linux为解决cpu 各自使用的L2 cache 数据与内存中的不一致的问题，，，per-CPU变量是linux系统一个非常有趣的特性，它为系统中的每个处理器都分配了该变量的副本。这样做的好处是，在多处理器系统中，当处理器操作属于它的变量副本时，不需要考虑与其他处理器的竞争的问题，同时该副本还可以充分利用处理器本地的硬件缓冲cache来提供访问速度。   
**RCU机制** 解决多个CPU同时读写共享数据的场景，，，，**随意读，但更新数据的时候，需要先复制一份副本，在副本上完成修改，再一次性地替换旧数据**   
**内存屏障** 程序运行过程中，对内存访问不一定按照代码编写的顺序来进行,,,读rmd()，，，写wmb()，，，既读又写md()
### I/O与网络模型
1. 阻塞&非阻塞
2. 多路复用
3. Signal IO
4. 异步IO
5. libvemt：事件驱动，如果事件发生，进行回调
1、4更适合块设备，2.3更适合字符设备   
      
为什么硬盘没有所谓的 多路复用，libevent，signal IO？   
因为select(串口), epoll（socket） 这些都是在监听事件，所以各种各样的IO模型，更多是描述字符设备和网络socket的问题。但硬盘的文件，只有读写，没有 epoll这些。 这些IO模型更多是在字符设备，网络socket的场景。

## Linux启动过程
### 改变
划分内核态和用户态   
实模式 ———>  保护模式
### 过程
### bootloader和bios

## 内存管理

## 进程管理

## 文件系统

## 网络协议栈

## 设备驱动
# 面经
# 算法